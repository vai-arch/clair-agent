{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ba435c5",
   "metadata": {},
   "source": [
    "Day 3 - Clair Agent\n",
    "Multi-source fusion: arXiv + Hacker News (no auth needed!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab9a8533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful\n",
      "üìÖ Date: 2025-11-20 09:25\n",
      "ü¶ô Model: llama3.2:3b\n",
      "üìä Sources: arXiv (5) + HN (10)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 1: Setup & Imports\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# Use __file__ for scripts, or Path of notebook if in Jupyter\n",
    "try:\n",
    "    base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    # __file__ does not exist in Jupyter, fall back to notebook path\n",
    "    base_dir = os.getcwd()\n",
    "\n",
    "# Add parent folder\n",
    "sys.path.append(os.path.dirname(base_dir))\n",
    "\n",
    "import arxiv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb import PersistentClient\n",
    "\n",
    "import config\n",
    "\n",
    "print(\"‚úÖ All imports successful\")\n",
    "print(f\"üìÖ Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"ü¶ô Model: {config.LLM_MODEL}\")\n",
    "print(f\"üìä Sources: arXiv ({config.MAX_PAPERS_PER_DAY}) + HN ({config.MAX_HN_STORIES})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbef6bfc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Initializing...\n",
      "\n",
      "‚úÖ All models initialized\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 2: Initialize Models\n",
    "# ============================================================\n",
    "\n",
    "print(\"üîß Initializing...\\n\")\n",
    "\n",
    "# LLM\n",
    "llm = OllamaLLM(\n",
    "    model=config.LLM_MODEL,\n",
    "    temperature=config.LLM_TEMPERATURE,\n",
    "    max_tokens=config.LLM_MAX_TOKENS\n",
    ")\n",
    "\n",
    "# Embedding model\n",
    "embed_model = SentenceTransformer(config.EMBED_MODEL)\n",
    "\n",
    "# ChromaDB\n",
    "CHROMA_DB_PATH = config.CHROMA_DIR\n",
    "chroma_client = PersistentClient(\n",
    "    path=CHROMA_DB_PATH,\n",
    "    settings=Settings(\n",
    "        anonymized_telemetry=False\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"‚úÖ All models initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48468a35",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Searching arXiv for up to 5 papers...\n",
      "‚úÖ Fetched 5 papers\n",
      "1. Tokenisation over Bounded Alphabets is Hard...\n",
      "2. In-N-On: Scaling Egocentric Manipulation with in-the-wild an...\n",
      "3. Think Visually, Reason Textually: Vision-Language Synergy in...\n",
      "4. Joint Semantic-Channel Coding and Modulation for Token Commu...\n",
      "5. RescueLens: LLM-Powered Triage and Action on Volunteer Feedb...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 3: Fetch arXiv Papers (Same as Day 2)\n",
    "# ============================================================\n",
    "import random\n",
    "from typing import List, Dict\n",
    "\n",
    "def fetch_papers(max_results=5, retries=5, base_delay=2) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch recent AI/ML papers from arXiv with retry logic for rate limiting (429)\n",
    "    and service errors (503).\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\nüîç Searching arXiv for up to {max_results} papers...\")\n",
    "\n",
    "    query = \" OR \".join([f\"cat:{cat}\" for cat in config.ARXIV_CATEGORIES])\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "\n",
    "    client = arxiv.Client()\n",
    "\n",
    "    attempt = 0\n",
    "    while attempt <= retries:\n",
    "        try:\n",
    "            papers = []\n",
    "            for paper in client.results(search):\n",
    "                papers.append({\n",
    "                    \"id\": paper.entry_id.split(\"/\")[-1],\n",
    "                    \"title\": paper.title,\n",
    "                    \"authors\": [a.name for a in paper.authors],\n",
    "                    \"summary\": paper.summary,\n",
    "                    \"url\": paper.entry_id,\n",
    "                    \"published\": paper.published,\n",
    "                    \"categories\": paper.categories,\n",
    "                    \"primary_category\": paper.primary_category,\n",
    "                    \"source\": \"arxiv\",\n",
    "                })\n",
    "\n",
    "            print(f\"‚úÖ Fetched {len(papers)} papers\")\n",
    "            return papers\n",
    "\n",
    "        except arxiv.HTTPError as e:\n",
    "            # retryable errors\n",
    "            if e.status in (429, 503):\n",
    "                wait = base_delay * (2 ** attempt) + random.uniform(0, 1)\n",
    "                print(f\"‚ö†Ô∏è arXiv error {e.status}. Retrying in {wait:.1f}s...\")\n",
    "                time.sleep(wait)\n",
    "                attempt += 1\n",
    "                continue\n",
    "\n",
    "            # unknown / non-retryable error ‚Üí raise immediately\n",
    "            raise\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Unexpected error: {e}\")\n",
    "            raise\n",
    "\n",
    "    print(\"‚ùå Failed to fetch papers after multiple retries.\")\n",
    "    return []\n",
    "\n",
    "papers = fetch_papers(config.MAX_PAPERS_PER_DAY)\n",
    "\n",
    "for i, p in enumerate(papers, 1):\n",
    "    print(f\"{i}. {p['title'][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "204a03f6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Searching Hacker News for 10 stories...\n",
      "‚úÖ Fetched 10 HN stories\n",
      "1. [1926‚Üë] Machine Learning Crash Course...\n",
      "2. [1656‚Üë] Machine Learning 101 slidedeck: 2 years of headbanging, so y...\n",
      "3. [1574‚Üë] John Carmack: I‚Äôm going to work on artificial general intell...\n",
      "4. [1487‚Üë] The Chaostron: An Important Advance in Learning Machines (19...\n",
      "5. [1069‚Üë] Apple's director of machine learning resigns due to return t...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 4: Fetch Hacker News Stories (NEW - NO AUTH!)\n",
    "# ============================================================\n",
    "\n",
    "def fetch_hacker_news_stories(max_stories=10):\n",
    "    \"\"\"\n",
    "    Fetch AI/ML-related stories from Hacker News\n",
    "    \n",
    "    Uses Algolia HN Search API (no authentication needed)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüîç Searching Hacker News for {max_stories} stories...\")\n",
    "    \n",
    "    stories = []\n",
    "    \n",
    "    # Search for AI/ML related stories\n",
    "    search_queries = [\n",
    "        \"artificial intelligence\",\n",
    "        \"machine learning\",\n",
    "        \"large language model\",\n",
    "        \"LLM\",\n",
    "        \"transformer\"\n",
    "    ]\n",
    "    \n",
    "    for query in search_queries[:2]:  # Just 2 queries to stay fast\n",
    "        try:\n",
    "            url = f\"{config.HN_SEARCH_API}/search\"\n",
    "            params = {\n",
    "                'query': query,\n",
    "                'tags': 'story',\n",
    "                'hitsPerPage': max_stories // 2,\n",
    "                'numericFilters': f'points>{config.HN_MIN_SCORE}'\n",
    "            }\n",
    "            \n",
    "            response = requests.get(url, params=params, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            for hit in data.get('hits', []):\n",
    "                # Avoid duplicates\n",
    "                if any(s['id'] == str(hit['objectID']) for s in stories):\n",
    "                    continue\n",
    "                \n",
    "                stories.append({\n",
    "                    'id': str(hit['objectID']),\n",
    "                    'title': hit.get('title', ''),\n",
    "                    'url': hit.get('url', f\"https://news.ycombinator.com/item?id={hit['objectID']}\"),\n",
    "                    'hn_url': f\"https://news.ycombinator.com/item?id={hit['objectID']}\",\n",
    "                    'score': hit.get('points', 0),\n",
    "                    'num_comments': hit.get('num_comments', 0),\n",
    "                    'author': hit.get('author', 'unknown'),\n",
    "                    'created': datetime.fromtimestamp(hit.get('created_at_i', 0)),\n",
    "                    'source': 'hackernews'\n",
    "                })\n",
    "                \n",
    "                if len(stories) >= max_stories:\n",
    "                    break\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error fetching HN for '{query}': {e}\")\n",
    "        \n",
    "        if len(stories) >= max_stories:\n",
    "            break\n",
    "    \n",
    "    # Sort by score (most upvoted first)\n",
    "    stories.sort(key=lambda x: x['score'], reverse=True)\n",
    "    stories = stories[:max_stories]\n",
    "    \n",
    "    print(f\"‚úÖ Fetched {len(stories)} HN stories\")\n",
    "    return stories\n",
    "\n",
    "hn_stories = fetch_hacker_news_stories(config.MAX_HN_STORIES)\n",
    "\n",
    "for i, story in enumerate(hn_stories[:5], 1):\n",
    "    print(f\"{i}. [{story['score']:3d}‚Üë] {story['title'][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c8df4df",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó Detecting cross-references...\n",
      "‚úÖ Found 0 cross-references\n",
      "   (No cross-references today - this is normal!)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 5: Cross-Reference Detection (NEW)\n",
    "# ============================================================\n",
    "\n",
    "def find_cross_references(papers, hn_stories):\n",
    "    \"\"\"\n",
    "    Detect when HN stories mention arXiv papers\n",
    "    \n",
    "    Methods:\n",
    "    1. arXiv ID in HN URL (e.g., arxiv.org/abs/2311.12345)\n",
    "    2. Title overlap (>50% words match)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nüîó Detecting cross-references...\")\n",
    "    \n",
    "    cross_refs = []\n",
    "    \n",
    "    for paper in papers:\n",
    "        paper_id = paper['id']\n",
    "        paper_title_words = set(paper['title'].lower().split())\n",
    "        \n",
    "        for story in hn_stories:\n",
    "            # Check 1: arXiv ID in HN URL\n",
    "            story_url = story['url'].lower()\n",
    "            story_title = story['title'].lower()\n",
    "            \n",
    "            if paper_id in story_url or 'arxiv.org' in story_url and paper_id.split('v')[0] in story_url:\n",
    "                cross_refs.append({\n",
    "                    'paper_id': paper_id,\n",
    "                    'paper_title': paper['title'],\n",
    "                    'hn_story_id': story['id'],\n",
    "                    'hn_title': story['title'],\n",
    "                    'hn_score': story['score'],\n",
    "                    'hn_comments': story['num_comments'],\n",
    "                    'match_type': 'arxiv_url'\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            # Check 2: Significant title overlap\n",
    "            story_title_words = set(story_title.split())\n",
    "            overlap = len(paper_title_words & story_title_words)\n",
    "            \n",
    "            if overlap >= len(paper_title_words) * 0.5 and len(paper_title_words) > 3:\n",
    "                cross_refs.append({\n",
    "                    'paper_id': paper_id,\n",
    "                    'paper_title': paper['title'],\n",
    "                    'hn_story_id': story['id'],\n",
    "                    'hn_title': story['title'],\n",
    "                    'hn_score': story['score'],\n",
    "                    'hn_comments': story['num_comments'],\n",
    "                    'match_type': 'title_overlap',\n",
    "                    'overlap_ratio': overlap / len(paper_title_words)\n",
    "                })\n",
    "    \n",
    "    print(f\"‚úÖ Found {len(cross_refs)} cross-references\")\n",
    "    \n",
    "    if cross_refs:\n",
    "        for ref in cross_refs:\n",
    "            print(f\"   Paper: {ref['paper_title'][:40]}...\")\n",
    "            print(f\"   ‚Üí HN ({ref['hn_score']}‚Üë, {ref['hn_comments']} comments): {ref['hn_title'][:40]}...\")\n",
    "    else:\n",
    "        print(\"   (No cross-references today - this is normal!)\")\n",
    "    \n",
    "    return cross_refs\n",
    "\n",
    "cross_refs = find_cross_references(papers, hn_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "710fc69b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Ranking with social signals...\n",
      "‚úÖ Ranking complete\n",
      "\n",
      "1. Score: 0.910 (base: 0.910, boost: 0.00)\n",
      "   In-N-On: Scaling Egocentric Manipulation with in-the-wild and on-task ...\n",
      "2. Score: 0.900 (base: 0.900, boost: 0.00)\n",
      "   Tokenisation over Bounded Alphabets is Hard...\n",
      "3. Score: 0.885 (base: 0.885, boost: 0.00)\n",
      "   Think Visually, Reason Textually: Vision-Language Synergy in ARC...\n",
      "4. Score: 0.885 (base: 0.885, boost: 0.00)\n",
      "   RescueLens: LLM-Powered Triage and Action on Volunteer Feedback for Fo...\n",
      "5. Score: 0.835 (base: 0.835, boost: 0.00)\n",
      "   Joint Semantic-Channel Coding and Modulation for Token Communications...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 6: Multi-Source Ranking with HN Signal (ENHANCED)\n",
    "# ============================================================\n",
    "\n",
    "def rank_with_social_signal(papers, cross_refs):\n",
    "    \"\"\"\n",
    "    Rank papers with HN mentions as social signal\n",
    "    \n",
    "    Formula: base_score √ó (1 + hn_boost)\n",
    "    HN boost = min(0.5, (points + comments/10) / 500)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nüìä Ranking with social signals...\")\n",
    "    \n",
    "    # Build cross-ref lookup\n",
    "    hn_mentions = {}\n",
    "    for ref in cross_refs:\n",
    "        paper_id = ref['paper_id']\n",
    "        if paper_id not in hn_mentions:\n",
    "            hn_mentions[paper_id] = []\n",
    "        hn_mentions[paper_id].append({\n",
    "            'score': ref['hn_score'],\n",
    "            'comments': ref['hn_comments']\n",
    "        })\n",
    "    \n",
    "    now = datetime.now(papers[0]['published'].tzinfo)\n",
    "    \n",
    "    ranked = []\n",
    "    for paper in papers:\n",
    "        # Base scoring (same as Day 2)\n",
    "        days_old = (now - paper['published']).days\n",
    "        recency_score = max(0, 1 - (days_old / 30))\n",
    "        \n",
    "        max_authors = max(len(p['authors']) for p in papers)\n",
    "        author_score = len(paper['authors']) / max_authors\n",
    "        \n",
    "        primary_cat = paper['primary_category']\n",
    "        if primary_cat in config.ARXIV_CATEGORIES:\n",
    "            relevance_score = 1.0\n",
    "        elif any(cat in config.ARXIV_CATEGORIES for cat in paper['categories']):\n",
    "            relevance_score = 0.7\n",
    "        else:\n",
    "            relevance_score = 0.3\n",
    "        \n",
    "        base_score = (\n",
    "            recency_score * config.RANK_WEIGHTS['recency'] +\n",
    "            author_score * config.RANK_WEIGHTS['authors'] +\n",
    "            relevance_score * config.RANK_WEIGHTS['relevance']\n",
    "        )\n",
    "        \n",
    "        # NEW: HN social signal boost\n",
    "        hn_boost = 0.0\n",
    "        total_hn_engagement = 0\n",
    "        if paper['id'] in hn_mentions:\n",
    "            for mention in hn_mentions[paper['id']]:\n",
    "                # Weight: upvotes + (comments / 10) for engagement\n",
    "                total_hn_engagement += mention['score'] + (mention['comments'] / 10)\n",
    "            \n",
    "            # Boost by 0-50% based on HN engagement\n",
    "            # 500 points = max boost\n",
    "            hn_boost = min(0.5, total_hn_engagement / 500)\n",
    "        \n",
    "        final_score = base_score * (1 + hn_boost)\n",
    "        \n",
    "        ranked.append({\n",
    "            **paper,\n",
    "            'scores': {\n",
    "                'recency': recency_score,\n",
    "                'authors': author_score,\n",
    "                'relevance': relevance_score,\n",
    "                'base': base_score,\n",
    "                'hn_boost': hn_boost,\n",
    "                'final': final_score\n",
    "            },\n",
    "            'hn_mentions': len(hn_mentions.get(paper['id'], [])),\n",
    "            'hn_engagement': total_hn_engagement\n",
    "        })\n",
    "    \n",
    "    ranked.sort(key=lambda x: x['scores']['final'], reverse=True)\n",
    "    \n",
    "    print(\"‚úÖ Ranking complete\\n\")\n",
    "    \n",
    "    for i, paper in enumerate(ranked, 1):\n",
    "        s = paper['scores']\n",
    "        mentions = f\" üî• HN: {paper['hn_engagement']:.0f} pts\" if paper['hn_mentions'] > 0 else \"\"\n",
    "        print(f\"{i}. Score: {s['final']:.3f} (base: {s['base']:.3f}, boost: {s['hn_boost']:.2f}){mentions}\")\n",
    "        print(f\"   {paper['title'][:70]}...\")\n",
    "    \n",
    "    return ranked\n",
    "\n",
    "ranked_papers = rank_with_social_signal(papers, cross_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd6117c7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßÆ Generating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cfb6179778746e4900585708a6f6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 5 embeddings (dim: 384)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 7: Generate Embeddings (Same as Day 2)\n",
    "# ============================================================\n",
    "\n",
    "def generate_embeddings(papers):\n",
    "    \"\"\"Generate embeddings for paper summaries\"\"\"\n",
    "    \n",
    "    print(\"\\nüßÆ Generating embeddings...\")\n",
    "    \n",
    "    texts = [\n",
    "        f\"{p['title']}. {p['summary'][:config.SUMMARY_TRUNCATE]}\"\n",
    "        for p in papers\n",
    "    ]\n",
    "    \n",
    "    embeddings = embed_model.encode(texts, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"‚úÖ Generated {len(embeddings)} embeddings (dim: {len(embeddings[0])})\")\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "embeddings = generate_embeddings(ranked_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc983c19",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Storing multi-source data...\n",
      "‚úÖ Stored 5 papers | Total: 10\n",
      "‚úÖ Stored 10 HN stories | Total: 10\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 8: Store Both Sources in ChromaDB (ENHANCED)\n",
    "# ============================================================\n",
    "\n",
    "def store_multi_source(papers, hn_stories, embed_model, chroma_client):\n",
    "    \"\"\"Store both arXiv and HN in separate collections\"\"\"\n",
    "    \n",
    "    print(\"\\nüíæ Storing multi-source data...\")\n",
    "    \n",
    "    # Collection 1: arXiv papers\n",
    "    try:\n",
    "        papers_collection = chroma_client.get_collection(\"arxiv_papers\")\n",
    "    except:\n",
    "        papers_collection = chroma_client.create_collection(\"arxiv_papers\")\n",
    "    \n",
    "    # Collection 2: Hacker News stories (NEW)\n",
    "    try:\n",
    "        hn_collection = chroma_client.get_collection(\"hackernews_stories\")\n",
    "    except:\n",
    "        hn_collection = chroma_client.create_collection(\"hackernews_stories\")\n",
    "    \n",
    "    # Store papers\n",
    "    if papers:\n",
    "        paper_texts = [f\"{p['title']}. {p['summary'][:500]}\" for p in papers]\n",
    "        paper_embeddings = embed_model.encode(paper_texts)\n",
    "        \n",
    "        papers_collection.upsert(\n",
    "            ids=[p['id'] for p in papers],\n",
    "            embeddings=paper_embeddings.tolist(),\n",
    "            documents=[p['summary'][:500] for p in papers],\n",
    "            metadatas=[\n",
    "                {\n",
    "                    'title': p['title'],\n",
    "                    'authors': ', '.join(p['authors'][:3]),\n",
    "                    'url': p['url'],\n",
    "                    'published': p['published'].strftime('%Y-%m-%d'),\n",
    "                    'rank_score': p['scores']['final'],\n",
    "                    'hn_mentions': p['hn_mentions']\n",
    "                }\n",
    "                for p in papers\n",
    "            ]\n",
    "        )\n",
    "        print(f\"‚úÖ Stored {len(papers)} papers | Total: {papers_collection.count()}\")\n",
    "    \n",
    "    # Store HN stories\n",
    "    if hn_stories:\n",
    "        hn_texts = [story['title'] for story in hn_stories]\n",
    "        hn_embeddings = embed_model.encode(hn_texts)\n",
    "        \n",
    "        hn_collection.upsert(\n",
    "            ids=[s['id'] for s in hn_stories],\n",
    "            embeddings=hn_embeddings.tolist(),\n",
    "            documents=[s['title'] for s in hn_stories],\n",
    "            metadatas=[\n",
    "                {\n",
    "                    'title': s['title'],\n",
    "                    'url': s['url'],\n",
    "                    'hn_url': s['hn_url'],\n",
    "                    'score': s['score'],\n",
    "                    'comments': s['num_comments'],\n",
    "                    'author': s['author'],\n",
    "                    'created': s['created'].strftime('%Y-%m-%d')\n",
    "                }\n",
    "                for s in hn_stories\n",
    "            ]\n",
    "        )\n",
    "        print(f\"‚úÖ Stored {len(hn_stories)} HN stories | Total: {hn_collection.count()}\")\n",
    "\n",
    "store_multi_source(ranked_papers, hn_stories, embed_model, chroma_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f73b1fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Multi-source search: 'most interesting recent AI research paper'\n",
      "‚úÖ Best paper: ARC Is a Vision Problem!...\n",
      "   HN mentions: 0\n",
      "‚úÖ Relevant HN context: 3 stories\n",
      "   [568‚Üë, 780 comments] Artificial intelligence is losing hype...\n",
      "   [1487‚Üë, 19 comments] The Chaostron: An Important Advance in Learning Ma...\n",
      "   [1574‚Üë, 889 comments] John Carmack: I‚Äôm going to work on artificial gene...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 9: Multi-Collection Semantic Search (ENHANCED)\n",
    "# ============================================================\n",
    "\n",
    "def search_across_sources(query, chroma_client, embed_model, top_k=3):\n",
    "    \"\"\"\n",
    "    Search both arXiv and HN collections\n",
    "    Return best paper + relevant HN context\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüîç Multi-source search: '{query}'\")\n",
    "    \n",
    "    query_embedding = embed_model.encode(query).tolist()\n",
    "    \n",
    "    # Search papers\n",
    "    papers_collection = chroma_client.get_collection(\"arxiv_papers\")\n",
    "    paper_results = papers_collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=1\n",
    "    )\n",
    "    \n",
    "    # Search HN\n",
    "    hn_collection = chroma_client.get_collection(\"hackernews_stories\")\n",
    "    hn_results = hn_collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    # Best paper\n",
    "    best_paper = None\n",
    "    if paper_results['ids'] and len(paper_results['ids'][0]) > 0:\n",
    "        meta = paper_results['metadatas'][0][0]\n",
    "\n",
    "        best_paper = {\n",
    "            'id': paper_results['ids'][0][0],\n",
    "            'title': meta.get('title'),\n",
    "            'summary': paper_results['documents'][0][0],\n",
    "            'url': meta.get('url'),\n",
    "            'authors': meta.get('authors', []),\n",
    "            'published': meta.get('published'),\n",
    "            'hn_mentions': meta.get('hn_mentions', 0),  # <-- SAFE DEFAULT\n",
    "        }\n",
    "\n",
    "        print(f\"‚úÖ Best paper: {best_paper['title'][:60]}...\")\n",
    "        print(f\"   HN mentions: {best_paper['hn_mentions']}\")\n",
    "    \n",
    "    # Relevant HN stories\n",
    "    relevant_hn = []\n",
    "    if hn_results['ids'] and len(hn_results['ids'][0]) > 0:\n",
    "        for i in range(len(hn_results['ids'][0])):\n",
    "            relevant_hn.append({\n",
    "                'title': hn_results['metadatas'][0][i]['title'],\n",
    "                'score': hn_results['metadatas'][0][i]['score'],\n",
    "                'comments': hn_results['metadatas'][0][i]['comments'],\n",
    "                'url': hn_results['metadatas'][0][i]['hn_url']\n",
    "            })\n",
    "        \n",
    "        print(f\"‚úÖ Relevant HN context: {len(relevant_hn)} stories\")\n",
    "        for story in relevant_hn:\n",
    "            print(f\"   [{story['score']:3d}‚Üë, {story['comments']} comments] {story['title'][:50]}...\")\n",
    "    \n",
    "    return best_paper, relevant_hn\n",
    "\n",
    "best_paper, relevant_hn = search_across_sources(\n",
    "    config.DAILY_QUERY,\n",
    "    chroma_client,\n",
    "    embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4334398c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 10: Generate Multi-Source Thread (ENHANCED)\n",
    "# ============================================================\n",
    "\n",
    "thread_template = \"\"\"You are a calm, technical AI researcher explaining papers clearly.\n",
    "\n",
    "Paper: {title}\n",
    "Authors: {authors}\n",
    "Summary: {summary}\n",
    "\n",
    "{hn_context}\n",
    "\n",
    "Write exactly 3 tweets about this paper. Rules:\n",
    "- Tweet 1: What problem this solves (under 250 chars)\n",
    "- Tweet 2: Key technical insight (under 250 chars)\n",
    "- Tweet 3: Why it matters (under 250 chars)\n",
    "{hn_instruction}\n",
    "- Be clear and technical, not hype\n",
    "- No buzzwords like \"revolutionary\" or \"game-changing\"\n",
    "\n",
    "Format your response EXACTLY like this:\n",
    "Tweet 1: [your text]\n",
    "Tweet 2: [your text]\n",
    "Tweet 3: [your text]\n",
    "\n",
    "Now write the 3 tweets:\"\"\"\n",
    "\n",
    "# Build HN context\n",
    "hn_context = \"\"\n",
    "hn_instruction = \"\"\n",
    "\n",
    "if best_paper['hn_mentions'] > 0 and relevant_hn:\n",
    "    total_engagement = sum(s['score'] + s['comments'] for s in relevant_hn[:2])\n",
    "    hn_context = f\"\\nHacker News Discussion: This topic has {total_engagement:.0f}+ points/comments on HN:\\n\"\n",
    "    for story in relevant_hn[:2]:\n",
    "        hn_context += f\"- [{story['score']}‚Üë, {story['comments']} comments] {story['title'][:60]}...\\n\"\n",
    "    hn_instruction = \"\\n- Mention HN discussion if relevant\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"title\", \"authors\", \"summary\", \"hn_context\", \"hn_instruction\"],\n",
    "    template=thread_template\n",
    ")\n",
    "\n",
    "print(\"\\nü§ñ Generating multi-source thread...\\n\")\n",
    "\n",
    "input_text = prompt.format(\n",
    "    title=best_paper['title'],\n",
    "    authors=best_paper['authors'],\n",
    "    summary=best_paper['summary'],\n",
    "    hn_context=hn_context,\n",
    "    hn_instruction=hn_instruction\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "thread = llm.invoke(input_text)\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(thread)\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n‚è±Ô∏è  Generated in {generation_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1835f388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 11: Save Thread with Source Attribution (ENHANCED)\n",
    "# ============================================================\n",
    "\n",
    "def save_thread(paper, hn_context, thread_content, gen_time, day=3):\n",
    "    \"\"\"Save thread with multi-source attribution\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = os.path.join(config.THREADS_DIR, f\"day{day:02d}_{timestamp}.md\")\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"# Day {day} Thread - Multi-Source (arXiv + HN)\\n\\n\")\n",
    "        f.write(f\"**Paper:** {paper['title']}\\n\")\n",
    "        f.write(f\"**Authors:** {paper['authors']}\\n\")\n",
    "        f.write(f\"**Published:** {paper['published']}\\n\")\n",
    "        f.write(f\"**URL:** {paper['url']}\\n\")\n",
    "        f.write(f\"**HN Mentions:** {paper['hn_mentions']}\\n\")\n",
    "        f.write(f\"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\")\n",
    "        f.write(f\"**Generation Time:** {gen_time:.1f}s\\n\\n\")\n",
    "        \n",
    "        if hn_context:\n",
    "            f.write(\"## Hacker News Context\\n\\n\")\n",
    "            f.write(hn_context)\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"---\\n\\n\")\n",
    "        f.write(thread_content)\n",
    "        f.write(\"\\n\\n---\\n\")\n",
    "        f.write(f\"*Generated by Clair Agent - Day {day}*\\n\")\n",
    "        f.write(\"*Stack: Ollama + LangChain + ChromaDB + Hacker News API*\\n\")\n",
    "        f.write(f\"*Sources: arXiv + HN | Cross-references: {len(cross_refs)}*\")\n",
    "    \n",
    "    return filename\n",
    "\n",
    "filename = save_thread(best_paper, hn_context, thread, generation_time, day=3)\n",
    "print(f\"\\nüíæ Thread saved to: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fafd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 12: Summary & Stats (ENHANCED)\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ DAY 3 COMPLETE - MULTI-SOURCE FUSION (arXiv + HN)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n‚úÖ arXiv papers: {len(papers)}\")\n",
    "print(f\"‚úÖ HN stories: {len(hn_stories)}\")\n",
    "print(f\"‚úÖ Cross-references: {len(cross_refs)}\")\n",
    "print(f\"‚úÖ Papers with HN mentions: {sum(1 for p in ranked_papers if p['hn_mentions'] > 0)}\")\n",
    "print(f\"‚úÖ Thread generated in {generation_time:.1f}s\")\n",
    "\n",
    "print(\"\\nüìä DATA STORED:\")\n",
    "papers_coll = chroma_client.get_collection(\"arxiv_papers\")\n",
    "hn_coll = chroma_client.get_collection(\"hackernews_stories\")\n",
    "print(f\"- arXiv papers in DB: {papers_coll.count()}\")\n",
    "print(f\"- HN stories in DB: {hn_coll.count()}\")\n",
    "\n",
    "print(\"\\nüéØ SELECTED PAPER:\")\n",
    "print(f\"Title: {best_paper['title'][:60]}...\")\n",
    "print(f\"HN mentions: {best_paper['hn_mentions']}\")\n",
    "print(f\"Selection: Multi-source semantic search\")\n",
    "\n",
    "print(\"\\nüí∞ COST: $0.00\")\n",
    "print(\"‚ú® BONUS: Zero authentication needed!\")\n",
    "\n",
    "print(\"\\nüîÆ TOMORROW (Day 4):\")\n",
    "print(\"- Add Hugging Face daily papers (3rd source)\")\n",
    "print(\"- 3-way cross-referencing\")\n",
    "print(\"- Human curation signal boost\")\n",
    "print(\"- Enhanced confidence scoring\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Total time today: ~90 minutes\")\n",
    "print(\"üí™ Multi-source intelligence with ZERO auth friction!\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "clair",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
