{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae086e7f",
   "metadata": {},
   "source": [
    "Day 4 - Clair Agent\n",
    "Triple-source fusion: arXiv + Hacker News + Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe6da89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 1: Setup & Imports\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import uuid\n",
    "import requests\n",
    "import random\n",
    "from typing import List, Dict\n",
    "from bs4 import BeautifulSoup\n",
    "from sympy import re\n",
    "\n",
    "try:\n",
    "    base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    base_dir = os.getcwd()\n",
    "\n",
    "sys.path.append(os.path.dirname(base_dir))\n",
    "\n",
    "import arxiv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb import PersistentClient\n",
    "\n",
    "import config\n",
    "\n",
    "print(\"‚úÖ All imports successful\")\n",
    "print(f\"üìÖ Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"ü¶ô Model: {config.LLM_MODEL}\")\n",
    "print(f\"üìä Sources: arXiv ({config.MAX_PAPERS_PER_DAY}) + HN ({config.MAX_HN_STORIES}) + HF ({config.MAX_HF_PAPERS})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd107fb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 2: Initialize Models\n",
    "# ============================================================\n",
    "\n",
    "print(\"üîß Initializing...\\n\")\n",
    "\n",
    "llm = OllamaLLM(\n",
    "    model=config.LLM_MODEL,\n",
    "    temperature=config.LLM_TEMPERATURE,\n",
    "    max_tokens=config.LLM_MAX_TOKENS\n",
    ")\n",
    "\n",
    "embed_model = SentenceTransformer(config.EMBED_MODEL)\n",
    "\n",
    "CHROMA_DB_PATH = config.CHROMA_DIR\n",
    "chroma_client = PersistentClient(\n",
    "    path=CHROMA_DB_PATH,\n",
    "    settings=Settings(anonymized_telemetry=False)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ All models initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301e5202",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3: Fetch arXiv Papers (Same as Day 3)\n",
    "# ============================================================\n",
    "\n",
    "def fetch_papers(max_results=5, retries=5, base_delay=2, days_back=3) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch recent AI/ML papers from arXiv with retry logic\n",
    "    \n",
    "    Args:\n",
    "        days_back: Look at papers from last N days (increases cross-ref chances)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüîç Searching arXiv for up to {max_results} papers (last {days_back} days)...\")\n",
    "    \n",
    "    # Fetch more papers, then filter by date\n",
    "    query = \" OR \".join([f\"cat:{cat}\" for cat in config.ARXIV_CATEGORIES])\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results * 3,  # Fetch 3x, filter later\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "    \n",
    "    client = arxiv.Client()\n",
    "    attempt = 0\n",
    "    \n",
    "    while attempt <= retries:\n",
    "        try:\n",
    "            papers = []\n",
    "            for paper in client.results(search):\n",
    "                papers.append({\n",
    "                    \"id\": paper.entry_id.split(\"/\")[-1],\n",
    "                    \"title\": paper.title,\n",
    "                    \"authors\": [a.name for a in paper.authors],\n",
    "                    \"summary\": paper.summary,\n",
    "                    \"url\": paper.entry_id,\n",
    "                    \"published\": paper.published,\n",
    "                    \"categories\": paper.categories,\n",
    "                    \"primary_category\": paper.primary_category,\n",
    "                    \"source\": \"arxiv\",\n",
    "                })\n",
    "            \n",
    "            print(f\"‚úÖ Fetched {len(papers)} papers\")\n",
    "            return papers\n",
    "        \n",
    "        except arxiv.HTTPError as e:\n",
    "            if e.status in (429, 503):\n",
    "                wait = base_delay * (2 ** attempt) + random.uniform(0, 1)\n",
    "                print(f\"‚ö†Ô∏è arXiv error {e.status}. Retrying in {wait:.1f}s...\")\n",
    "                time.sleep(wait)\n",
    "                attempt += 1\n",
    "                continue\n",
    "            raise\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Unexpected error: {e}\")\n",
    "            raise\n",
    "    \n",
    "    print(\"‚ùå Failed to fetch papers after multiple retries.\")\n",
    "    return []\n",
    "\n",
    "papers = fetch_papers(config.MAX_PAPERS_PER_DAY)\n",
    "\n",
    "for i, p in enumerate(papers, 1):\n",
    "    print(f\"{i}. {p['title'][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b96a79e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 4: Fetch Hacker News Stories (Same as Day 3)\n",
    "# ============================================================\n",
    "\n",
    "def fetch_hacker_news_stories(max_stories=10):\n",
    "    \"\"\"Fetch AI/ML-related stories from Hacker News\"\"\"\n",
    "    \n",
    "    print(f\"\\nüîç Searching Hacker News for {max_stories} stories...\")\n",
    "    \n",
    "    stories = []\n",
    "    search_queries = [\"artificial intelligence\", \"machine learning\"]\n",
    "    \n",
    "    for query in search_queries[:2]:\n",
    "        try:\n",
    "            url = f\"{config.HN_SEARCH_API}/search\"\n",
    "            params = {\n",
    "                'query': query,\n",
    "                'tags': 'story',\n",
    "                'hitsPerPage': max_stories // 2,\n",
    "                'numericFilters': f'points>{config.HN_MIN_SCORE}'\n",
    "            }\n",
    "            \n",
    "            response = requests.get(url, params=params, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            for hit in data.get('hits', []):\n",
    "                if any(s['id'] == str(hit['objectID']) for s in stories):\n",
    "                    continue\n",
    "                \n",
    "                stories.append({\n",
    "                    'id': str(hit['objectID']),\n",
    "                    'title': hit.get('title', ''),\n",
    "                    'url': hit.get('url', f\"https://news.ycombinator.com/item?id={hit['objectID']}\"),\n",
    "                    'hn_url': f\"https://news.ycombinator.com/item?id={hit['objectID']}\",\n",
    "                    'score': hit.get('points', 0),\n",
    "                    'num_comments': hit.get('num_comments', 0),\n",
    "                    'author': hit.get('author', 'unknown'),\n",
    "                    'created': datetime.fromtimestamp(hit.get('created_at_i', 0)),\n",
    "                    'source': 'hackernews'\n",
    "                })\n",
    "                \n",
    "                if len(stories) >= max_stories:\n",
    "                    break\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error fetching HN for '{query}': {e}\")\n",
    "        \n",
    "        if len(stories) >= max_stories:\n",
    "            break\n",
    "    \n",
    "    stories.sort(key=lambda x: x['score'], reverse=True)\n",
    "    stories = stories[:max_stories]\n",
    "    \n",
    "    print(f\"‚úÖ Fetched {len(stories)} HN stories\")\n",
    "    return stories\n",
    "\n",
    "hn_stories = fetch_hacker_news_stories(config.MAX_HN_STORIES)\n",
    "\n",
    "for i, story in enumerate(hn_stories[:5], 1):\n",
    "    print(f\"{i}. [{story['score']:3d}‚Üë] {story['title'][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f662a6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 5: Fetch Hugging Face Daily Papers (NEW!)\n",
    "# ============================================================\n",
    "\n",
    "def fetch_huggingface_papers(max_papers=10):\n",
    "    \"\"\"\n",
    "    Scrape Hugging Face Daily Papers.\n",
    "\n",
    "    Returns a list of dicts with: id, title, url, hf_url, upvotes, source, featured_date\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç Scraping Hugging Face for {max_papers} papers...\")\n",
    "\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        response = requests.get(config.HF_DAILY_PAPERS_URL, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "        papers = []\n",
    "        # HF Daily Papers uses <article> tags for each paper\n",
    "        paper_cards = soup.find_all('article', limit=max_papers)\n",
    "\n",
    "        for card in paper_cards:\n",
    "            try:\n",
    "                # Title\n",
    "                title_elem = card.find('h3') or card.find('h2')\n",
    "                if not title_elem:\n",
    "                    continue\n",
    "                title = title_elem.get_text(strip=True)\n",
    "\n",
    "                # HF link\n",
    "                link_elem = card.find('a', href=True)\n",
    "                if not link_elem:\n",
    "                    continue\n",
    "                hf_link = 'https://huggingface.co' + link_elem['href']\n",
    "\n",
    "                # Extract arXiv ID from link if present\n",
    "                arxiv_id = None\n",
    "                if '/papers/' in hf_link:\n",
    "                    raw = hf_link.split('/papers/')[-1]\n",
    "                    raw = raw.split('?')[0].strip()   # remove query params\n",
    "                    raw = raw.replace(\"v1\", \"\").replace(\"v2\", \"\").replace(\"v3\", \"\")\n",
    "                    if re.match(r\"^\\d{4}\\.\\d{4,5}$\", raw):\n",
    "                        arxiv_id = raw\n",
    "\n",
    "                # Upvotes\n",
    "                upvotes = 0\n",
    "                vote_wrapper = card.find('div', class_=lambda x: x and 'shadow-alternate' in x)\n",
    "                if vote_wrapper:\n",
    "                    vote_div = vote_wrapper.find('div', class_='leading-none')\n",
    "                    if vote_div:\n",
    "                        try:\n",
    "                            upvotes = int(vote_div.get_text(strip=True))\n",
    "                        except:\n",
    "                            upvotes = 0\n",
    "\n",
    "                papers.append({\n",
    "                    'id': arxiv_id or hf_link or str(uuid.uuid4()),\n",
    "                    'title': title,\n",
    "                    'url': hf_link,\n",
    "                    'hf_url': hf_link,\n",
    "                    'upvotes': upvotes,\n",
    "                    'source': 'huggingface',\n",
    "                    'featured_date': datetime.now().strftime('%Y-%m-%d')\n",
    "                })\n",
    "\n",
    "                if len(papers) >= max_papers:\n",
    "                    break\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error parsing card: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"‚úÖ Fetched {len(papers)} HF papers\")\n",
    "        if papers:\n",
    "            for i, p in enumerate(papers[:5], 1):\n",
    "                print(f\"{i}. [{p['upvotes']:2d}ü§ó] {p['title'][:60]}...\")\n",
    "\n",
    "        return papers\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error scraping Hugging Face: {e}\")\n",
    "        return []\n",
    "\n",
    "# Usage\n",
    "hf_papers = fetch_huggingface_papers(config.MAX_HF_PAPERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5025e419",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 6: 3-Way Cross-Reference Detection (ENHANCED!)\n",
    "# ============================================================\n",
    "\n",
    "def find_triple_cross_references(papers, hn_stories, hf_papers):\n",
    "    \"\"\"\n",
    "    Detect cross-references across ALL 3 sources\n",
    "    \n",
    "    Returns:\n",
    "        - arxiv_hn: Papers mentioned on HN\n",
    "        - arxiv_hf: Papers featured on HF\n",
    "        - triple_hits: Papers on all 3 platforms (GOLD!)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nüîó Detecting 3-way cross-references...\")\n",
    "    \n",
    "    # Build lookup sets\n",
    "    hf_paper_ids = {p['id'] for p in hf_papers if p.get('id')}\n",
    "    \n",
    "    arxiv_hn = []\n",
    "    arxiv_hf = []\n",
    "    triple_hits = []\n",
    "    \n",
    "    for paper in papers:\n",
    "        paper_id = paper['id']\n",
    "        paper_title_words = set(paper['title'].lower().split())\n",
    "        \n",
    "        is_on_hf = paper_id in hf_paper_ids\n",
    "        is_on_hn = False\n",
    "        hn_match = None\n",
    "        \n",
    "        # Check HN mentions\n",
    "        for story in hn_stories:\n",
    "            story_url = story['url'].lower()\n",
    "            story_title = story['title'].lower()\n",
    "            \n",
    "            # Direct arXiv URL match\n",
    "            if paper_id in story_url or ('arxiv.org' in story_url and paper_id.split('v')[0] in story_url):\n",
    "                is_on_hn = True\n",
    "                hn_match = story\n",
    "                arxiv_hn.append({\n",
    "                    'paper_id': paper_id,\n",
    "                    'paper_title': paper['title'],\n",
    "                    'hn_story_id': story['id'],\n",
    "                    'hn_title': story['title'],\n",
    "                    'hn_score': story['score'],\n",
    "                    'hn_comments': story['num_comments'],\n",
    "                    'match_type': 'arxiv_url'\n",
    "                })\n",
    "                break\n",
    "            \n",
    "            # Title overlap\n",
    "            story_title_words = set(story_title.split())\n",
    "            overlap = len(paper_title_words & story_title_words)\n",
    "            \n",
    "            if overlap >= len(paper_title_words) * 0.5 and len(paper_title_words) > 3:\n",
    "                is_on_hn = True\n",
    "                hn_match = story\n",
    "                arxiv_hn.append({\n",
    "                    'paper_id': paper_id,\n",
    "                    'paper_title': paper['title'],\n",
    "                    'hn_story_id': story['id'],\n",
    "                    'hn_title': story['title'],\n",
    "                    'hn_score': story['score'],\n",
    "                    'hn_comments': story['num_comments'],\n",
    "                    'match_type': 'title_overlap'\n",
    "                })\n",
    "                break\n",
    "        \n",
    "        # Check HF featuring\n",
    "        if is_on_hf:\n",
    "            hf_match = next((p for p in hf_papers if p['id'] == paper_id), None)\n",
    "            arxiv_hf.append({\n",
    "                'paper_id': paper_id,\n",
    "                'paper_title': paper['title'],\n",
    "                'hf_upvotes': hf_match['upvotes'] if hf_match else 0\n",
    "            })\n",
    "        \n",
    "        # TRIPLE HIT (on all 3 platforms!)\n",
    "        if is_on_hn and is_on_hf:\n",
    "            triple_hits.append({\n",
    "                'paper_id': paper_id,\n",
    "                'paper_title': paper['title'],\n",
    "                'hn_score': hn_match['score'] if hn_match else 0,\n",
    "                'hn_comments': hn_match['num_comments'] if hn_match else 0,\n",
    "                'hf_upvotes': hf_match['upvotes'] if hf_match else 0\n",
    "            })\n",
    "    \n",
    "    print(f\"‚úÖ arXiv ‚Üî HN: {len(arxiv_hn)}\")\n",
    "    print(f\"‚úÖ arXiv ‚Üî HF: {len(arxiv_hf)}\")\n",
    "    print(f\"üèÜ TRIPLE HITS (arXiv + HN + HF): {len(triple_hits)}\")\n",
    "    \n",
    "    if triple_hits:\n",
    "        print(\"\\nüî• Papers on ALL 3 platforms:\")\n",
    "        for hit in triple_hits:\n",
    "            print(f\"   ‚Ä¢ {hit['paper_title'][:50]}...\")\n",
    "            print(f\"     HN: {hit['hn_score']}‚Üë | HF: {hit['hf_upvotes']}ü§ó\")\n",
    "    \n",
    "    return {\n",
    "        'arxiv_hn': arxiv_hn,\n",
    "        'arxiv_hf': arxiv_hf,\n",
    "        'triple_hits': triple_hits\n",
    "    }\n",
    "\n",
    "cross_refs = find_triple_cross_references(papers, hn_stories, hf_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a366325",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 7: Triple-Boost Ranking (ENHANCED!)\n",
    "# ============================================================\n",
    "\n",
    "def rank_with_triple_signal(papers, cross_refs):\n",
    "    \"\"\"\n",
    "    Rank with 3 social signals\n",
    "    \n",
    "    Formula: base_score √ó (1 + hn_boost) √ó (1 + hf_boost)\n",
    "    \n",
    "    Triple hits get MASSIVE boost\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nüìä Ranking with triple social signals...\")\n",
    "    \n",
    "    # Build lookups\n",
    "    hn_mentions = {}\n",
    "    for ref in cross_refs['arxiv_hn']:\n",
    "        paper_id = ref['paper_id']\n",
    "        if paper_id not in hn_mentions:\n",
    "            hn_mentions[paper_id] = []\n",
    "        hn_mentions[paper_id].append({\n",
    "            'score': ref['hn_score'],\n",
    "            'comments': ref['hn_comments']\n",
    "        })\n",
    "    \n",
    "    hf_mentions = {}\n",
    "    for ref in cross_refs['arxiv_hf']:\n",
    "        paper_id = ref['paper_id']\n",
    "        hf_mentions[paper_id] = ref['hf_upvotes']\n",
    "    \n",
    "    triple_hit_ids = {hit['paper_id'] for hit in cross_refs['triple_hits']}\n",
    "    \n",
    "    now = datetime.now(papers[0]['published'].tzinfo)\n",
    "    ranked = []\n",
    "    \n",
    "    for paper in papers:\n",
    "        # Base scoring\n",
    "        days_old = (now - paper['published']).days\n",
    "        recency_score = max(0, 1 - (days_old / 30))\n",
    "        \n",
    "        max_authors = max(len(p['authors']) for p in papers)\n",
    "        author_score = len(paper['authors']) / max_authors\n",
    "        \n",
    "        primary_cat = paper['primary_category']\n",
    "        if primary_cat in config.ARXIV_CATEGORIES:\n",
    "            relevance_score = 1.0\n",
    "        elif any(cat in config.ARXIV_CATEGORIES for cat in paper['categories']):\n",
    "            relevance_score = 0.7\n",
    "        else:\n",
    "            relevance_score = 0.3\n",
    "        \n",
    "        base_score = (\n",
    "            recency_score * config.RANK_WEIGHTS['recency'] +\n",
    "            author_score * config.RANK_WEIGHTS['authors'] +\n",
    "            relevance_score * config.RANK_WEIGHTS['relevance']\n",
    "        )\n",
    "        \n",
    "        # HN boost\n",
    "        hn_boost = 0.0\n",
    "        total_hn_engagement = 0\n",
    "        if paper['id'] in hn_mentions:\n",
    "            for mention in hn_mentions[paper['id']]:\n",
    "                total_hn_engagement += mention['score'] + (mention['comments'] / 10)\n",
    "            hn_boost = min(0.5, total_hn_engagement / 500)\n",
    "        \n",
    "        # HF boost (NEW!)\n",
    "        hf_boost = 0.0\n",
    "        if paper['id'] in hf_mentions:\n",
    "            # HF curation = strong signal\n",
    "            # Base boost 0.3 just for being featured\n",
    "            # + up to 0.2 for upvotes\n",
    "            hf_upvotes = hf_mentions[paper['id']]\n",
    "            hf_boost = 0.3 + min(0.2, hf_upvotes / 100)\n",
    "        \n",
    "        # Triple hit bonus (NEW!)\n",
    "        triple_bonus = 1.2 if paper['id'] in triple_hit_ids else 1.0\n",
    "        \n",
    "        # Final score with multiplicative boosts\n",
    "        final_score = base_score * (1 + hn_boost) * (1 + hf_boost) * triple_bonus\n",
    "        \n",
    "        ranked.append({\n",
    "            **paper,\n",
    "            'scores': {\n",
    "                'recency': recency_score,\n",
    "                'authors': author_score,\n",
    "                'relevance': relevance_score,\n",
    "                'base': base_score,\n",
    "                'hn_boost': hn_boost,\n",
    "                'hf_boost': hf_boost,\n",
    "                'triple_bonus': triple_bonus,\n",
    "                'final': final_score\n",
    "            },\n",
    "            'hn_mentions': len(hn_mentions.get(paper['id'], [])),\n",
    "            'hn_engagement': total_hn_engagement,\n",
    "            'hf_featured': paper['id'] in hf_mentions,\n",
    "            'hf_upvotes': hf_mentions.get(paper['id'], 0),\n",
    "            'triple_hit': paper['id'] in triple_hit_ids\n",
    "        })\n",
    "    \n",
    "    ranked.sort(key=lambda x: x['scores']['final'], reverse=True)\n",
    "    \n",
    "    print(\"‚úÖ Ranking complete\\n\")\n",
    "    \n",
    "    for i, paper in enumerate(ranked, 1):\n",
    "        s = paper['scores']\n",
    "        badges = []\n",
    "        if paper['hn_mentions'] > 0:\n",
    "            badges.append(f\"HN:{paper['hn_engagement']:.0f}\")\n",
    "        if paper['hf_featured']:\n",
    "            badges.append(f\"HF:{paper['hf_upvotes']}ü§ó\")\n",
    "        if paper['triple_hit']:\n",
    "            badges.append(\"üèÜTRIPLE\")\n",
    "        \n",
    "        badge_str = f\" [{', '.join(badges)}]\" if badges else \"\"\n",
    "        \n",
    "        print(f\"{i}. Score: {s['final']:.3f}{badge_str}\")\n",
    "        print(f\"   {paper['title'][:70]}...\")\n",
    "    \n",
    "    return ranked\n",
    "\n",
    "ranked_papers = rank_with_triple_signal(papers, cross_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45686200",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 8: Generate Embeddings\n",
    "# ============================================================\n",
    "\n",
    "def generate_embeddings(papers):\n",
    "    \"\"\"Generate embeddings for paper summaries\"\"\"\n",
    "    \n",
    "    print(\"\\nüßÆ Generating embeddings...\")\n",
    "    \n",
    "    texts = [\n",
    "        f\"{p['title']}. {p['summary'][:config.SUMMARY_TRUNCATE]}\"\n",
    "        for p in papers\n",
    "    ]\n",
    "    \n",
    "    embeddings = embed_model.encode(texts, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"‚úÖ Generated {len(embeddings)} embeddings (dim: {len(embeddings[0])})\")\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "embeddings = generate_embeddings(ranked_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f10721",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 9: Store Triple-Source in ChromaDB (ENHANCED!)\n",
    "# ============================================================\n",
    "\n",
    "def store_triple_source(papers, hn_stories, hf_papers, embed_model, chroma_client):\n",
    "    \"\"\"Store all 3 sources in separate collections\"\"\"\n",
    "    \n",
    "    print(\"\\nüíæ Storing triple-source data...\")\n",
    "    \n",
    "    # Collection 1: arXiv papers\n",
    "    try:\n",
    "        papers_collection = chroma_client.get_collection(\"arxiv_papers\")\n",
    "    except:\n",
    "        papers_collection = chroma_client.create_collection(\"arxiv_papers\")\n",
    "    \n",
    "    # Collection 2: Hacker News\n",
    "    try:\n",
    "        hn_collection = chroma_client.get_collection(\"hackernews_stories\")\n",
    "    except:\n",
    "        hn_collection = chroma_client.create_collection(\"hackernews_stories\")\n",
    "    \n",
    "    # Collection 3: Hugging Face (NEW!)\n",
    "    try:\n",
    "        hf_collection = chroma_client.get_collection(\"huggingface_papers\")\n",
    "    except:\n",
    "        hf_collection = chroma_client.create_collection(\"huggingface_papers\")\n",
    "    \n",
    "    # Store arXiv papers\n",
    "    if papers:\n",
    "        paper_texts = [f\"{p['title']}. {p['summary'][:500]}\" for p in papers]\n",
    "        paper_embeddings = embed_model.encode(paper_texts)\n",
    "        \n",
    "        papers_collection.upsert(\n",
    "            ids=[p['id'] for p in papers],\n",
    "            embeddings=paper_embeddings.tolist(),\n",
    "            documents=[p['summary'][:500] for p in papers],\n",
    "            metadatas=[\n",
    "                {\n",
    "                    'title': p['title'],\n",
    "                    'authors': ', '.join(p['authors'][:3]),\n",
    "                    'url': p['url'],\n",
    "                    'published': p['published'].strftime('%Y-%m-%d'),\n",
    "                    'rank_score': p['scores']['final'],\n",
    "                    'hn_mentions': p['hn_mentions'],\n",
    "                    'hf_featured': p['hf_featured'],\n",
    "                    'triple_hit': p['triple_hit']\n",
    "                }\n",
    "                for p in papers\n",
    "            ]\n",
    "        )\n",
    "        print(f\"‚úÖ Stored {len(papers)} papers | Total: {papers_collection.count()}\")\n",
    "    \n",
    "    # Store HN stories\n",
    "    if hn_stories:\n",
    "        hn_texts = [story['title'] for story in hn_stories]\n",
    "        hn_embeddings = embed_model.encode(hn_texts)\n",
    "        \n",
    "        hn_collection.upsert(\n",
    "            ids=[s['id'] for s in hn_stories],\n",
    "            embeddings=hn_embeddings.tolist(),\n",
    "            documents=[s['title'] for s in hn_stories],\n",
    "            metadatas=[\n",
    "                {\n",
    "                    'title': s['title'],\n",
    "                    'url': s['url'],\n",
    "                    'hn_url': s['hn_url'],\n",
    "                    'score': s['score'],\n",
    "                    'comments': s['num_comments'],\n",
    "                    'author': s['author'],\n",
    "                    'created': s['created'].strftime('%Y-%m-%d')\n",
    "                }\n",
    "                for s in hn_stories\n",
    "            ]\n",
    "        )\n",
    "        print(f\"‚úÖ Stored {len(hn_stories)} HN stories | Total: {hn_collection.count()}\")\n",
    "    \n",
    "    # Store HF papers (NEW!)\n",
    "    if hf_papers:\n",
    "        hf_texts = [p['title'] for p in hf_papers]\n",
    "        hf_embeddings = embed_model.encode(hf_texts)\n",
    "        \n",
    "        hf_collection.upsert(\n",
    "            ids=[str(p.get(\"id\") or p[\"url\"] or uuid.uuid4()) for p in hf_papers],\n",
    "            embeddings=hf_embeddings.tolist(),\n",
    "            documents=[p['title'] for p in hf_papers],\n",
    "            metadatas=[\n",
    "                {\n",
    "                    'title': p['title'],\n",
    "                    'url': p['url'],\n",
    "                    'hf_url': p['hf_url'],\n",
    "                    'upvotes': p['upvotes'],\n",
    "                    'featured_date': p['featured_date']\n",
    "                }\n",
    "                for p in hf_papers\n",
    "            ]\n",
    "        )\n",
    "        print(f\"‚úÖ Stored {len(hf_papers)} HF papers | Total: {hf_collection.count()}\")\n",
    "\n",
    "store_triple_source(ranked_papers, hn_stories, hf_papers, embed_model, chroma_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530ed629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 10: Triple-Source Semantic Search (ENHANCED!)\n",
    "# ============================================================\n",
    "\n",
    "def search_across_triple_sources(query, chroma_client, embed_model, top_k=3):\n",
    "    \"\"\"Search all 3 collections\"\"\"\n",
    "    \n",
    "    print(f\"\\nüîç Triple-source search: '{query}'\")\n",
    "    \n",
    "    query_embedding = embed_model.encode(query).tolist()\n",
    "    \n",
    "    # Search arXiv\n",
    "    papers_collection = chroma_client.get_collection(\"arxiv_papers\")\n",
    "    if papers_collection.count() == 0:\n",
    "        print(\"‚ö†Ô∏è arxiv_papers collection is empty ‚Äî skipping search\")\n",
    "        paper_results = {\"ids\": [[]], \"documents\": [[]], \"metadatas\": [[]]}\n",
    "    else:\n",
    "        paper_results = papers_collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=1\n",
    "        )\n",
    "    \n",
    "    # Search HN\n",
    "    hn_collection = chroma_client.get_collection(\"hackernews_stories\")\n",
    "    if hn_collection.count() == 0:\n",
    "        print(\"‚ö†Ô∏è hackernews_stories collection is empty ‚Äî skipping search\")\n",
    "        hn_results = {\"ids\": [[]], \"documents\": [[]], \"metadatas\": [[]]}\n",
    "    else:\n",
    "        hn_results = hn_collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=top_k\n",
    "        )\n",
    "    \n",
    "    # Search HF\n",
    "    hf_collection = chroma_client.get_collection(\"huggingface_papers\")\n",
    "    if hf_collection.count() == 0:\n",
    "        print(\"‚ö†Ô∏è huggingface_papers collection is empty ‚Äî skipping search\")\n",
    "        hf_results = {\"ids\": [[]], \"documents\": [[]], \"metadatas\": [[]]}\n",
    "    else:\n",
    "        hf_results = hf_collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    # Best paper\n",
    "    best_paper = None\n",
    "    if paper_results['ids'] and len(paper_results['ids'][0]) > 0:\n",
    "        meta = paper_results['metadatas'][0][0]\n",
    "        \n",
    "        best_paper = {\n",
    "            'id': paper_results['ids'][0][0],\n",
    "            'title': meta.get('title'),\n",
    "            'summary': paper_results['documents'][0][0],\n",
    "            'url': meta.get('url'),\n",
    "            'authors': meta.get('authors', []),\n",
    "            'published': meta.get('published'),\n",
    "            'hn_mentions': meta.get('hn_mentions', 0),\n",
    "            'hf_featured': meta.get('hf_featured', False),\n",
    "            'triple_hit': meta.get('triple_hit', False)\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Best paper: {best_paper['title'][:60]}...\")\n",
    "        print(f\"   HN mentions: {best_paper['hn_mentions']}\")\n",
    "        print(f\"   HF featured: {best_paper['hf_featured']}\")\n",
    "        if best_paper['triple_hit']:\n",
    "            print(f\"   üèÜ TRIPLE HIT!\")\n",
    "    \n",
    "    # Relevant context\n",
    "    relevant_hn = []\n",
    "    if hn_results['ids'] and len(hn_results['ids'][0]) > 0:\n",
    "        for i in range(len(hn_results['ids'][0])):\n",
    "            relevant_hn.append({\n",
    "                'title': hn_results['metadatas'][0][i]['title'],\n",
    "                'score': hn_results['metadatas'][0][i]['score'],\n",
    "                'comments': hn_results['metadatas'][0][i]['comments'],\n",
    "                'url': hn_results['metadatas'][0][i]['hn_url']\n",
    "            })\n",
    "    \n",
    "    relevant_hf = []\n",
    "    if hf_results['ids'] and len(hf_results['ids'][0]) > 0:\n",
    "        for i in range(len(hf_results['ids'][0])):\n",
    "            relevant_hf.append({\n",
    "                'title': hf_results['metadatas'][0][i]['title'],\n",
    "                'upvotes': hf_results['metadatas'][0][i]['upvotes'],\n",
    "                'url': hf_results['metadatas'][0][i]['hf_url']\n",
    "            })\n",
    "    \n",
    "    return best_paper, relevant_hn, relevant_hf\n",
    "\n",
    "best_paper, relevant_hn, relevant_hf = search_across_triple_sources(\n",
    "    config.DAILY_QUERY,\n",
    "    chroma_client,\n",
    "    embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc5e0e0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 11: Generate Triple-Source Thread (ENHANCED!)\n",
    "# ============================================================\n",
    "\n",
    "thread_template = \"\"\"You are a calm, technical AI researcher explaining papers clearly.\n",
    "\n",
    "Paper: {title}\n",
    "Authors: {authors}\n",
    "Summary: {summary}\n",
    "\n",
    "{context}\n",
    "\n",
    "Write exactly 3 tweets about this paper. Rules:\n",
    "- Tweet 1: What problem this solves (under 250 chars)\n",
    "- Tweet 2: Key technical insight (under 250 chars)\n",
    "- Tweet 3: Why it matters (under 250 chars)\n",
    "{instruction}\n",
    "- Be clear and technical, not hype\n",
    "- No buzzwords\n",
    "\n",
    "Format EXACTLY:\n",
    "Tweet 1: [your text]\n",
    "Tweet 2: [your text]\n",
    "Tweet 3: [your text]\n",
    "\n",
    "Now write the 3 tweets:\"\"\"\n",
    "\n",
    "# Build context\n",
    "context = \"\"\n",
    "instruction = \"\"\n",
    "\n",
    "if best_paper['triple_hit']:\n",
    "    context = f\"\\nüèÜ TRIPLE VALIDATION: This paper is trending on arXiv, Hacker News, AND Hugging Face!\\n\"\n",
    "    instruction = \"\\n- Mention multi-platform validation\"\n",
    "elif best_paper['hn_mentions'] > 0 and best_paper['hf_featured']:\n",
    "    context = f\"\\nMulti-platform signal: Featured on Hugging Face + trending on HN\\n\"\n",
    "    instruction = \"\\n- Mention cross-platform interest\"\n",
    "elif best_paper['hf_featured']:\n",
    "    context = f\"\\nHuman-curated: Featured on Hugging Face daily papers\\n\"\n",
    "    instruction = \"\\n- Mention HF curation\"\n",
    "elif best_paper['hn_mentions'] > 0 and relevant_hn:\n",
    "    total_engagement = sum(s['score'] + s['comments'] for s in relevant_hn[:2])\n",
    "    context = f\"\\nTrending on Hacker News with {total_engagement:.0f}+ points/comments\\n\"\n",
    "    instruction = \"\\n- Mention HN discussion\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"title\", \"authors\", \"summary\", \"context\", \"instruction\"],\n",
    "    template=thread_template\n",
    ")\n",
    "\n",
    "print(\"\\nü§ñ Generating triple-source thread...\\n\")\n",
    "\n",
    "input_text = prompt.format(\n",
    "    title=best_paper['title'],\n",
    "    authors=best_paper['authors'],\n",
    "    summary=best_paper['summary'],\n",
    "    context=context,\n",
    "    instruction=instruction\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "import time\n",
    "\n",
    "for attempt in range(5):\n",
    "    try:\n",
    "        thread = llm.invoke(input_text)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Attempt {attempt+1} failed: {e}\")\n",
    "        time.sleep(5)\n",
    "else:\n",
    "    print(\"LLM invoke failed after 5 attempts\")\n",
    "\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(thread)\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n‚è±Ô∏è  Generated in {generation_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8027ba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 12: Save Thread with Triple Attribution\n",
    "# ============================================================\n",
    "\n",
    "def save_thread(paper, context, thread_content, gen_time, cross_refs, day=4):\n",
    "    \"\"\"Save thread with triple-source attribution\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = os.path.join(config.THREADS_DIR, f\"day{day:02d}_{timestamp}.md\")\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"# Day {day} Thread - Triple Source (arXiv + HN + HF)\\n\\n\")\n",
    "        f.write(f\"**Paper:** {paper['title']}\\n\")\n",
    "        f.write(f\"**Authors:** {paper['authors']}\\n\")\n",
    "        f.write(f\"**Published:** {paper['published']}\\n\")\n",
    "        f.write(f\"**URL:** {paper['url']}\\n\")\n",
    "        f.write(f\"**HN Mentions:** {paper['hn_mentions']}\\n\")\n",
    "        f.write(f\"**HF Featured:** {'Yes' if paper['hf_featured'] else 'No'}\\n\")\n",
    "        \n",
    "        if paper['triple_hit']:\n",
    "            f.write(f\"**üèÜ TRIPLE HIT:** Found on all 3 platforms!\\n\")\n",
    "        \n",
    "        f.write(f\"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\")\n",
    "        f.write(f\"**Generation Time:** {gen_time:.1f}s\\n\\n\")\n",
    "        \n",
    "        if context:\n",
    "            f.write(\"## Multi-Platform Context\\n\\n\")\n",
    "            f.write(context)\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"---\\n\\n\")\n",
    "        f.write(thread_content)\n",
    "        f.write(\"\\n\\n---\\n\")\n",
    "        f.write(f\"*Generated by Clair Agent - Day {day}*\\n\")\n",
    "        f.write(\"*Stack: Ollama + LangChain + ChromaDB + HN API + HF Scraping*\\n\")\n",
    "        f.write(f\"*Sources: arXiv + HN + HF*\\n\")\n",
    "        f.write(f\"*Cross-references: {len(cross_refs['arxiv_hn'])} HN + {len(cross_refs['arxiv_hf'])} HF + {len(cross_refs['triple_hits'])} Triple*\")\n",
    "    \n",
    "    return filename\n",
    "\n",
    "filename = save_thread(best_paper, context, thread, generation_time, cross_refs, day=4)\n",
    "print(f\"\\nüíæ Thread saved to: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce02be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 13: Summary & Stats\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ DAY 4 COMPLETE - TRIPLE-SOURCE FUSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n‚úÖ arXiv papers: {len(papers)}\")\n",
    "print(f\"‚úÖ HN stories: {len(hn_stories)}\")\n",
    "print(f\"‚úÖ HF papers: {len(hf_papers)}\")\n",
    "print(f\"‚úÖ arXiv ‚Üî HN cross-refs: {len(cross_refs['arxiv_hn'])}\")\n",
    "print(f\"‚úÖ arXiv ‚Üî HF cross-refs: {len(cross_refs['arxiv_hf'])}\")\n",
    "print(f\"üèÜ Triple hits (all 3 platforms): {len(cross_refs['triple_hits'])}\")\n",
    "print(f\"‚úÖ Thread generated in {generation_time:.1f}s\")\n",
    "\n",
    "print(\"\\nüìä DATA STORED:\")\n",
    "papers_coll = chroma_client.get_collection(\"arxiv_papers\")\n",
    "hn_coll = chroma_client.get_collection(\"hackernews_stories\")\n",
    "hf_coll = chroma_client.get_collection(\"huggingface_papers\")\n",
    "print(f\"- arXiv papers in DB: {papers_coll.count()}\")\n",
    "print(f\"- HN stories in DB: {hn_coll.count()}\")\n",
    "print(f\"- HF papers in DB: {hf_coll.count()}\")\n",
    "\n",
    "print(\"\\nüéØ SELECTED PAPER:\")\n",
    "print(f\"Title: {best_paper['title'][:60]}...\")\n",
    "print(f\"HN mentions: {best_paper['hn_mentions']}\")\n",
    "print(f\"HF featured: {best_paper['hf_featured']}\")\n",
    "print(f\"Triple hit: {'üèÜ YES!' if best_paper['triple_hit'] else 'No'}\")\n",
    "\n",
    "print(\"\\nüí° INSIGHT:\")\n",
    "if cross_refs['triple_hits']:\n",
    "    print(\"Papers on all 3 platforms = HIGHEST quality signal\")\n",
    "    print(\"These papers have: academic rigor + technical crowd + human curation\")\n",
    "else:\n",
    "    print(\"No triple hits today - that's rare but normal!\")\n",
    "    print(\"Multi-platform cross-referencing still provides strong signals\")\n",
    "\n",
    "print(\"\\nüí∞ COST: $0.00\")\n",
    "print(\"‚ú® BONUS: 3 sources, zero authentication!\")\n",
    "\n",
    "print(\"\\nüìã TODO NOW:\")\n",
    "print(\"1. Read thread in threads/day04_*.md\")\n",
    "print(\"2. Post to X with multi-platform attribution\")\n",
    "print(\"3. Build-in-public update\")\n",
    "print(\"4. Commit to GitHub\")\n",
    "\n",
    "print(\"\\nüîÆ TOMORROW (Day 5):\")\n",
    "print(\"- Add X/Twitter scraping (4th source)\")\n",
    "print(\"- 4-way cross-referencing\")\n",
    "print(\"- Virality score across all platforms\")\n",
    "print(\"- First 'confidence score' (0-100%)\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Total time today: ~90 minutes\")\n",
    "print(\"üí™ Triple-source intelligence = near-perfect signal detection!\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
