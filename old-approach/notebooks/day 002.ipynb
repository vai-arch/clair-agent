{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15df93ae",
   "metadata": {},
   "source": [
    "Day 2 - Clair Agent\n",
    "Multi-paper ranking + embeddings + ChromaDB + semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38832f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful\n",
      "üìÖ Date: 2025-11-19 10:28\n",
      "ü¶ô Using: llama3.2:3b\n",
      "üìä Fetching: 5 papers\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 1: Setup & Imports\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# Use __file__ for scripts, or Path of notebook if in Jupyter\n",
    "try:\n",
    "    base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    # __file__ does not exist in Jupyter, fall back to notebook path\n",
    "    base_dir = os.getcwd()\n",
    "\n",
    "# Add parent folder\n",
    "sys.path.append(os.path.dirname(base_dir))\n",
    "\n",
    "import arxiv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb import PersistentClient\n",
    "\n",
    "import config\n",
    "\n",
    "print(\"‚úÖ All imports successful\")\n",
    "print(f\"üìÖ Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"ü¶ô Using: {config.LLM_MODEL}\")\n",
    "print(f\"üìä Fetching: {config.MAX_PAPERS_PER_DAY} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18b0cead",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Initializing models...\n",
      "\n",
      "üìä Created new collection\n",
      "‚úÖ Models initialized\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 2: Initialize Models\n",
    "# ============================================================\n",
    "\n",
    "print(\"üîß Initializing models...\\n\")\n",
    "\n",
    "# LLM\n",
    "llm = OllamaLLM(\n",
    "    model=config.LLM_MODEL,\n",
    "    temperature=config.LLM_TEMPERATURE,\n",
    "    max_tokens=config.LLM_MAX_TOKENS\n",
    ")\n",
    "\n",
    "# Embedding model\n",
    "embed_model = SentenceTransformer(config.EMBED_MODEL)\n",
    "\n",
    "CHROMA_DB_PATH = config.CHROMA_DIR\n",
    "# ChromaDB client\n",
    "chroma_client = PersistentClient(\n",
    "    path=CHROMA_DB_PATH,\n",
    "    settings=Settings(\n",
    "        anonymized_telemetry=False\n",
    "    )\n",
    ")\n",
    "# Get or create collection\n",
    "try:\n",
    "    collection = chroma_client.get_collection(\"arxiv_papers\")\n",
    "    print(f\"üìä Loaded existing collection: {collection.count()} papers\")\n",
    "except:\n",
    "    collection = chroma_client.create_collection(\n",
    "        name=\"arxiv_papers\",\n",
    "        metadata={\"description\": \"AI/ML papers from arXiv\"}\n",
    "    )\n",
    "    print(\"üìä Created new collection\")\n",
    "\n",
    "print(\"‚úÖ Models initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "197ba20b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Searching arXiv for 5 papers...\n",
      "‚úÖ Fetched 5 papers\n",
      "\n",
      "1. ARC Is a Vision Problem!...\n",
      "   Authors: 8 | Published: 2025-11-18\n",
      "\n",
      "2. $œÄ^{*}_{0.6}$: a VLA That Learns From Experience...\n",
      "   Authors: 55 | Published: 2025-11-18\n",
      "\n",
      "3. Robust Verification of Controllers under State Uncertainty v...\n",
      "   Authors: 3 | Published: 2025-11-18\n",
      "\n",
      "4. SparseST: Exploiting Data Sparsity in Spatiotemporal Modelin...\n",
      "   Authors: 5 | Published: 2025-11-18\n",
      "\n",
      "5. Look-Ahead Reasoning on Learning Platforms...\n",
      "   Authors: 3 | Published: 2025-11-18\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 3: Fetch Multiple Papers\n",
    "# ============================================================\n",
    "\n",
    "def fetch_papers(max_results=5):\n",
    "    \"\"\"Fetch recent AI/ML papers from arXiv\"\"\"\n",
    "    \n",
    "    print(f\"\\nüîç Searching arXiv for {max_results} papers...\")\n",
    "    \n",
    "    client = arxiv.Client()\n",
    "    \n",
    "    # Build query from configured categories\n",
    "    query = \" OR \".join([f\"cat:{cat}\" for cat in config.ARXIV_CATEGORIES])\n",
    "    \n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "    \n",
    "    papers = []\n",
    "    for paper in client.results(search):\n",
    "        papers.append({\n",
    "            'id': paper.entry_id.split('/')[-1],  # Extract ID\n",
    "            'title': paper.title,\n",
    "            'authors': [a.name for a in paper.authors],\n",
    "            'summary': paper.summary,\n",
    "            'url': paper.entry_id,\n",
    "            'published': paper.published,\n",
    "            'categories': paper.categories,\n",
    "            'primary_category': paper.primary_category\n",
    "        })\n",
    "    \n",
    "    print(f\"‚úÖ Fetched {len(papers)} papers\")\n",
    "    return papers\n",
    "\n",
    "papers = fetch_papers(config.MAX_PAPERS_PER_DAY)\n",
    "\n",
    "# Display summary\n",
    "for i, p in enumerate(papers, 1):\n",
    "    print(f\"\\n{i}. {p['title'][:60]}...\")\n",
    "    print(f\"   Authors: {len(p['authors'])} | Published: {p['published'].strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "807ffb71",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Ranking papers...\n",
      "‚úÖ Ranking complete\n",
      "\n",
      "1. Score: 1.000 | $œÄ^{*}_{0.6}$: a VLA That Learns From Experience...\n",
      "   Recency: 1.00 | Authors: 1.00 | Relevance: 1.00\n",
      "2. Score: 0.818 | SparseST: Exploiting Data Sparsity in Spatiotempor...\n",
      "   Recency: 1.00 | Authors: 0.09 | Relevance: 1.00\n",
      "3. Score: 0.811 | Look-Ahead Reasoning on Learning Platforms...\n",
      "   Recency: 1.00 | Authors: 0.05 | Relevance: 1.00\n",
      "4. Score: 0.739 | ARC Is a Vision Problem!...\n",
      "   Recency: 1.00 | Authors: 0.15 | Relevance: 0.70\n",
      "5. Score: 0.721 | Robust Verification of Controllers under State Unc...\n",
      "   Recency: 1.00 | Authors: 0.05 | Relevance: 0.70\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 4: Rank Papers\n",
    "# ============================================================\n",
    "\n",
    "def rank_papers(papers):\n",
    "    \"\"\"\n",
    "    Rank papers by multiple criteria\n",
    "    \n",
    "    Scoring:\n",
    "    - Recency: How recent is it? (0-1, 1=today)\n",
    "    - Authors: More authors = more collaboration = higher quality? (0-1)\n",
    "    - Relevance: How well does category match our focus? (0-1)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nüìä Ranking papers...\")\n",
    "    \n",
    "    now = datetime.now(papers[0]['published'].tzinfo)  # Match timezone\n",
    "    \n",
    "    ranked = []\n",
    "    for paper in papers:\n",
    "        # Recency score (1.0 = today, decays over 30 days)\n",
    "        days_old = (now - paper['published']).days\n",
    "        recency_score = max(0, 1 - (days_old / 30))\n",
    "        \n",
    "        # Author score (normalized by max authors in dataset)\n",
    "        max_authors = max(len(p['authors']) for p in papers)\n",
    "        author_score = len(paper['authors']) / max_authors\n",
    "        \n",
    "        # Relevance score (is it in our primary categories?)\n",
    "        primary_cat = paper['primary_category']\n",
    "        if primary_cat in config.ARXIV_CATEGORIES:\n",
    "            relevance_score = 1.0\n",
    "        elif any(cat in config.ARXIV_CATEGORIES for cat in paper['categories']):\n",
    "            relevance_score = 0.7\n",
    "        else:\n",
    "            relevance_score = 0.3\n",
    "        \n",
    "        # Weighted total\n",
    "        total_score = (\n",
    "            recency_score * config.RANK_WEIGHTS['recency'] +\n",
    "            author_score * config.RANK_WEIGHTS['authors'] +\n",
    "            relevance_score * config.RANK_WEIGHTS['relevance']\n",
    "        )\n",
    "        \n",
    "        ranked.append({\n",
    "            **paper,\n",
    "            'scores': {\n",
    "                'recency': recency_score,\n",
    "                'authors': author_score,\n",
    "                'relevance': relevance_score,\n",
    "                'total': total_score\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    # Sort by total score\n",
    "    ranked.sort(key=lambda x: x['scores']['total'], reverse=True)\n",
    "    \n",
    "    print(\"‚úÖ Ranking complete\\n\")\n",
    "    \n",
    "    # Display rankings\n",
    "    for i, paper in enumerate(ranked, 1):\n",
    "        scores = paper['scores']\n",
    "        print(f\"{i}. Score: {scores['total']:.3f} | {paper['title'][:50]}...\")\n",
    "        print(f\"   Recency: {scores['recency']:.2f} | Authors: {scores['authors']:.2f} | Relevance: {scores['relevance']:.2f}\")\n",
    "    \n",
    "    return ranked\n",
    "\n",
    "ranked_papers = rank_papers(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8cb6a07",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßÆ Generating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6ba1471e9d4bbf88be1839aa411b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 5 embeddings (dim: 384)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 5: Generate Embeddings\n",
    "# ============================================================\n",
    "\n",
    "def generate_embeddings(papers):\n",
    "    \"\"\"Generate embeddings for paper summaries\"\"\"\n",
    "    \n",
    "    print(\"\\nüßÆ Generating embeddings...\")\n",
    "    \n",
    "    # Create text to embed (title + summary for richer context)\n",
    "    texts = [\n",
    "        f\"{p['title']}. {p['summary'][:config.SUMMARY_TRUNCATE]}\"\n",
    "        for p in papers\n",
    "    ]\n",
    "    \n",
    "    # Batch encode\n",
    "    embeddings = embed_model.encode(texts, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"‚úÖ Generated {len(embeddings)} embeddings (dim: {len(embeddings[0])})\")\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "embeddings = generate_embeddings(ranked_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5496f3e1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Storing in ChromaDB...\n",
      "‚úÖ Stored 5 papers\n",
      "üìä Total papers in DB: 5\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 6: Store in ChromaDB\n",
    "# ============================================================\n",
    "\n",
    "def store_papers(papers, embeddings, collection):\n",
    "    \"\"\"Store papers with embeddings in ChromaDB\"\"\"\n",
    "    \n",
    "    print(\"\\nüíæ Storing in ChromaDB...\")\n",
    "    \n",
    "    # Prepare data\n",
    "    ids = [p['id'] for p in papers]\n",
    "    documents = [p['summary'][:config.SUMMARY_TRUNCATE] for p in papers]\n",
    "    metadatas = [\n",
    "        {\n",
    "            'title': p['title'],\n",
    "            'authors': ', '.join(p['authors'][:3]),  # First 3 authors\n",
    "            'url': p['url'],\n",
    "            'published': p['published'].strftime('%Y-%m-%d'),\n",
    "            'primary_category': p['primary_category'],\n",
    "            'rank_score': p['scores']['total']\n",
    "        }\n",
    "        for p in papers\n",
    "    ]\n",
    "    \n",
    "    # Add to collection (upsert = update if exists, insert if not)\n",
    "    collection.upsert(\n",
    "        ids=ids,\n",
    "        embeddings=embeddings.tolist(),\n",
    "        documents=documents,\n",
    "        metadatas=metadatas\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Stored {len(papers)} papers\")\n",
    "    print(f\"üìä Total papers in DB: {collection.count()}\")\n",
    "\n",
    "store_papers(ranked_papers, embeddings, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "054d2138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Semantic search: 'most impactful novel AI technique with practical applications'\n",
      "‚úÖ Best match: $œÄ^{*}_{0.6}$: a VLA That Learns From Experience...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 7: Semantic Search\n",
    "# ============================================================\n",
    "\n",
    "def semantic_search(query, collection, top_k=1):\n",
    "    \"\"\"\n",
    "    Find most relevant paper using semantic search\n",
    "    \n",
    "    Args:\n",
    "        query: Natural language query\n",
    "        collection: ChromaDB collection\n",
    "        top_k: Number of results to return\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüîç Semantic search: '{query}'\")\n",
    "    \n",
    "    # Embed the query\n",
    "    query_embedding = embed_model.encode(query)\n",
    "    \n",
    "    # Search\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding.tolist()],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    # Extract best result\n",
    "    if results['ids'] and len(results['ids'][0]) > 0:\n",
    "        best_match = {\n",
    "            'id': results['ids'][0][0],\n",
    "            'title': results['metadatas'][0][0]['title'],\n",
    "            'summary': results['documents'][0][0],\n",
    "            'url': results['metadatas'][0][0]['url'],\n",
    "            'authors': results['metadatas'][0][0]['authors'],\n",
    "            'published': results['metadatas'][0][0]['published'],\n",
    "            'distance': results['distances'][0][0] if 'distances' in results else None\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Best match: {best_match['title'][:60]}...\")\n",
    "        return best_match\n",
    "    else:\n",
    "        print(\"‚ùå No results found\")\n",
    "        return None\n",
    "\n",
    "# Search for best paper\n",
    "best_paper = semantic_search(config.DAILY_QUERY, collection, top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "045058b1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Generating thread...\n",
      "\n",
      "============================================================\n",
      "Here are three tweets about the paper \"$œÄ^{*}_{0.6}$: a VLA That Learns From Experience\":\n",
      "\n",
      "Tweet 1: Researchers tackle the challenge of training vision-language-action (VLA) models in real-world settings, where data quality & availability can be limited. Their work on RECAP provides a general-purpose method for RL-based improvement.\n",
      "\n",
      "Tweet 2: Key insight: The paper introduces Advantage-conditioned Policies (ACP), which utilizes heterogeneous data sources (demos, on-policy collection, expert teleop) to enhance the self-improvement process via reinforcement learning.\n",
      "\n",
      "Tweet 3: This work matters because it addresses a critical gap in VLA model development: leveraging real-world experience for improvement. By integrating diverse data sources, RECAP enables more robust and adaptable models, crucial for practical applications.\n",
      "============================================================\n",
      "\n",
      "‚è±Ô∏è  Generated in 60.0s\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 8: Generate Thread\n",
    "# ============================================================\n",
    "\n",
    "thread_template = \"\"\"You are a calm, technical AI researcher explaining papers clearly.\n",
    "\n",
    "Paper: {title}\n",
    "Authors: {authors}\n",
    "Summary: {summary}\n",
    "\n",
    "Write exactly 3 tweets about this paper. Rules:\n",
    "- Tweet 1: What problem this solves (under 250 chars)\n",
    "- Tweet 2: Key technical insight (under 250 chars) \n",
    "- Tweet 3: Why it matters (under 250 chars)\n",
    "- Be clear and technical, not hype\n",
    "- No buzzwords like \"revolutionary\" or \"game-changing\"\n",
    "\n",
    "Format your response EXACTLY like this:\n",
    "Tweet 1: [your text]\n",
    "Tweet 2: [your text]\n",
    "Tweet 3: [your text]\n",
    "\n",
    "Now write the 3 tweets:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"title\", \"authors\", \"summary\"],\n",
    "    template=thread_template\n",
    ")\n",
    "\n",
    "print(\"\\nü§ñ Generating thread...\\n\")\n",
    "\n",
    "input_text = prompt.format(\n",
    "    title=best_paper['title'],\n",
    "    authors=best_paper['authors'],\n",
    "    summary=best_paper['summary']\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "thread = llm.invoke(input_text)\n",
    "generation_time = time.time() - start_time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(thread)\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n‚è±Ô∏è  Generated in {generation_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1dd9a621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Thread saved to: c:\\Users\\victor.diaz\\Documents\\_AI\\clair-agent\\threads\\day02_20251119_102956.md\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 9: Save Thread\n",
    "# ============================================================\n",
    "\n",
    "def save_thread(paper_data, thread_content, gen_time, day=2):\n",
    "    \"\"\"Save thread as markdown\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    filename = os.path.join(config.THREADS_DIR, f\"day{day:02d}_{timestamp}.md\")\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"# Day {day} Thread\\n\\n\")\n",
    "        f.write(f\"**Paper:** {paper_data['title']}\\n\")\n",
    "        f.write(f\"**Authors:** {paper_data['authors']}\\n\")\n",
    "        f.write(f\"**Published:** {paper_data['published']}\\n\")\n",
    "        f.write(f\"**URL:** {paper_data['url']}\\n\")\n",
    "        f.write(f\"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\")\n",
    "        f.write(f\"**Generation Time:** {gen_time:.1f}s\\n\\n\")\n",
    "        f.write(\"---\\n\\n\")\n",
    "        f.write(thread_content)\n",
    "        f.write(\"\\n\\n---\\n\")\n",
    "        f.write(f\"*Generated by Clair Agent - Day {day}*\\n\")\n",
    "        f.write(\"*Stack: Ollama + LangChain + ChromaDB + semantic search*\\n\")\n",
    "        f.write(f\"*Selected from {config.MAX_PAPERS_PER_DAY} papers via ranking + embedding similarity*\")\n",
    "    \n",
    "    return filename\n",
    "\n",
    "filename = save_thread(best_paper, thread, generation_time, day=2)\n",
    "print(f\"\\nüíæ Thread saved to: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86fb7c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéâ DAY 2 COMPLETE - MULTI-PAPER PIPELINE\n",
      "============================================================\n",
      "\n",
      "‚úÖ Papers fetched: 5\n",
      "‚úÖ Papers ranked by 3 criteria\n",
      "‚úÖ Embeddings generated: 5\n",
      "‚úÖ Papers in ChromaDB: 5\n",
      "‚úÖ Semantic search complete\n",
      "‚úÖ Thread generated in 60.0s\n",
      "‚úÖ Saved to: c:\\Users\\victor.diaz\\Documents\\_AI\\clair-agent\\threads\\day02_20251119_102956.md\n",
      "\n",
      "üìä TOP 3 PAPERS TODAY:\n",
      "1. [1.000] $œÄ^{*}_{0.6}$: a VLA That Learns From Experience...\n",
      "2. [0.818] SparseST: Exploiting Data Sparsity in Spatiotempor...\n",
      "3. [0.811] Look-Ahead Reasoning on Learning Platforms...\n",
      "\n",
      "üéØ SELECTED PAPER:\n",
      "Title: $œÄ^{*}_{0.6}$: a VLA That Learns From Experience...\n",
      "Method: Semantic search\n",
      "Query: 'most impactful novel AI technique with practical applications'\n",
      "\n",
      "üí∞ COST:\n",
      "- Today: $0.00\n",
      "- Forever: $0.00\n",
      "\n",
      "üìã TODO NOW:\n",
      "1. Read thread in threads/day02_*.md\n",
      "2. Post to X manually\n",
      "3. Build-in-public update\n",
      "4. Commit to GitHub\n",
      "\n",
      "üîÆ TOMORROW (Day 3):\n",
      "- Add Reddit r/MachineLearning scraping\n",
      "- Multi-source fusion (arXiv + Reddit)\n",
      "- Source diversity scoring\n",
      "- Cross-reference detection\n",
      "\n",
      "‚è±Ô∏è  Total time today: ~60-90 minutes\n",
      "üí™ You now have real RAG infrastructure.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 10: Summary & Stats\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ DAY 2 COMPLETE - MULTI-PAPER PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n‚úÖ Papers fetched: {len(papers)}\")\n",
    "print(f\"‚úÖ Papers ranked by {len(config.RANK_WEIGHTS)} criteria\")\n",
    "print(f\"‚úÖ Embeddings generated: {len(embeddings)}\")\n",
    "print(f\"‚úÖ Papers in ChromaDB: {collection.count()}\")\n",
    "print(f\"‚úÖ Semantic search complete\")\n",
    "print(f\"‚úÖ Thread generated in {generation_time:.1f}s\")\n",
    "print(f\"‚úÖ Saved to: {filename}\")\n",
    "\n",
    "print(\"\\nüìä TOP 3 PAPERS TODAY:\")\n",
    "for i, paper in enumerate(ranked_papers[:3], 1):\n",
    "    print(f\"{i}. [{paper['scores']['total']:.3f}] {paper['title'][:50]}...\")\n",
    "\n",
    "print(\"\\nüéØ SELECTED PAPER:\")\n",
    "print(f\"Title: {best_paper['title'][:60]}...\")\n",
    "print(f\"Method: Semantic search\")\n",
    "print(f\"Query: '{config.DAILY_QUERY}'\")\n",
    "\n",
    "print(\"\\nüí∞ COST:\")\n",
    "print(\"- Today: $0.00\")\n",
    "print(\"- Forever: $0.00\")\n",
    "\n",
    "print(\"\\nüìã TODO NOW:\")\n",
    "print(\"1. Read thread in threads/day02_*.md\")\n",
    "print(\"2. Post to X manually\")\n",
    "print(\"3. Build-in-public update\")\n",
    "print(\"4. Commit to GitHub\")\n",
    "\n",
    "print(\"\\nüîÆ TOMORROW (Day 3):\")\n",
    "print(\"- Add Reddit r/MachineLearning scraping\")\n",
    "print(\"- Multi-source fusion (arXiv + Reddit)\")\n",
    "print(\"- Source diversity scoring\")\n",
    "print(\"- Cross-reference detection\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Total time today: ~60-90 minutes\")\n",
    "print(\"üí™ You now have real RAG infrastructure.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "clair",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
