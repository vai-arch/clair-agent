# Day 2 Thread

**Paper:** $π^{*}_{0.6}$: a VLA That Learns From Experience
**Authors:** Ali Amin, Raichelle Aniceto, Ashwin Balakrishna
**Published:** 2025-11-18
**URL:** http://arxiv.org/abs/2511.14759v1
**Generated:** 2025-11-19 10:29
**Generation Time:** 60.0s

---

Here are three tweets about the paper "$π^{*}_{0.6}$: a VLA That Learns From Experience":

Tweet 1: Researchers tackle the challenge of training vision-language-action (VLA) models in real-world settings, where data quality & availability can be limited. Their work on RECAP provides a general-purpose method for RL-based improvement.

Tweet 2: Key insight: The paper introduces Advantage-conditioned Policies (ACP), which utilizes heterogeneous data sources (demos, on-policy collection, expert teleop) to enhance the self-improvement process via reinforcement learning.

Tweet 3: This work matters because it addresses a critical gap in VLA model development: leveraging real-world experience for improvement. By integrating diverse data sources, RECAP enables more robust and adaptable models, crucial for practical applications.

---
*Generated by Clair Agent - Day 2*
*Stack: Ollama + LangChain + ChromaDB + semantic search*
*Selected from 5 papers via ranking + embedding similarity*